{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89we7QYnLtay",
        "outputId": "2c4e29ba-dbc9-4a70-d89a-abf3a18f7534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install sagemaker --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "sess = sagemaker.Session()\n",
        "# sagemaker session bucket -> used for uploading data, models and logs\n",
        "# sagemaker will automatically create this bucket if it not exists\n",
        "sagemaker_session_bucket=None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    # set to default bucket if a bucket name is not given\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "try:\n",
        "    role = sagemaker.get_execution_role()\n",
        "except ValueError:\n",
        "    iam = boto3.client('iam')\n",
        "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
        "\n",
        "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
        "\n",
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4SO4yLDVL12H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests as r\n",
        "\n",
        "lighteval_version = \"0.2.0\"\n",
        "\n",
        "# create scripts directory if not exists\n",
        "os.makedirs(\"scripts\", exist_ok=True)\n",
        "\n",
        "# load custom scripts from git\n",
        "raw_github_url = f\"https://raw.githubusercontent.com/huggingface/lighteval/v{lighteval_version}/run_evals_accelerate.py\"\n",
        "res = r.get(raw_github_url)\n",
        "with open(\"scripts/run_evals_accelerate.py\", \"w\") as f:\n",
        "    f.write(res.text)\n",
        "\n",
        "# write requirements.txt\n",
        "with open(\"scripts/requirements.txt\", \"w\") as f:\n",
        "    f.write(f\"lighteval=={lighteval_version}\")\n"
      ],
      "metadata": {
        "id": "2nWy4kguL53G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "We are going to evaluate the model on the Truthfulqa benchmark with 0 few-shot examples. TruthfulQA is a benchmark designed to measure whether a language model generates truthful answers to questions, encompassing 817 questions across 38 categories including health, law, finance, and politics​​."
      ],
      "metadata": {
        "id": "BQt7JdzcML-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"scripts/tasks.txt\", \"w\") as f:\n",
        "    f.write(f\"lighteval|truthfulqa:mc|0|0\")"
      ],
      "metadata": {
        "id": "gseIYnAPL56f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "Evaluate Zephyr 7B on TruthfulQA on Amazon SageMaker\n",
        "\n",
        "In this example we are going to evaluate the HuggingFaceH4/zephyr-7b-beta on the MMLU benchmark, which is part of the Open LLM Leaderboard.\n",
        "\n",
        "In addition to the task argument we need to define:\n",
        "\n",
        "    model_args: Hugging Face Model ID or path, defined as pretrained=HuggingFaceH4/zephyr-7b-beta\n",
        "    model_dtype: The model data type, defined as bfloat16, float16 or float32\n",
        "    output_dir: The directory where the evaluation results will be saved, e.g. /opt/ml/model"
      ],
      "metadata": {
        "id": "Dh1hRptnMo7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import HuggingFace\n",
        "\n",
        "# hyperparameters, which are passed into the training job\n",
        "hyperparameters = {\n",
        "  'model_args': \"pretrained=HuggingFaceH4/zephyr-7b-beta\", # Hugging Face Model ID\n",
        "  'task': 'tasks.txt',  # 'lighteval|truthfulqa:mc|0|0',\n",
        "  'model_dtype': 'bfloat16', # Torch dtype to load model weights\n",
        "  'output_dir': '/opt/ml/model' # Directory, which sagemaker uploads to s3 after training\n",
        "}\n",
        "\n",
        "# create the Estimator\n",
        "huggingface_estimator = HuggingFace(\n",
        "    entry_point          = 'run_evals_accelerate.py',      # train script\n",
        "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
        "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
        "    instance_count       = 1,                 # the number of instances used for training\n",
        "    base_job_name        = \"lighteval\",       # the name of the training job\n",
        "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
        "    volume_size          = 300,               # the size of the EBS volume in GB\n",
        "    transformers_version = '4.36',            # the transformers version used in the training job\n",
        "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
        "    py_version           = 'py310',           # the python version used in the training job\n",
        "    hyperparameters      =  hyperparameters,\n",
        "    environment          = {\n",
        "                            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",\n",
        "                            \"HF_TOKEN\": \"REPALCE_WITH_YOUR_TOKEN\" # needed for private models\n",
        "                            }, # set env variable to cache models in /tmp\n",
        ")\n",
        "\n",
        "\n",
        "# starting the train job with our uploaded datasets as input\n",
        "huggingface_estimator.fit()"
      ],
      "metadata": {
        "id": "sHgYl7rhL593"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "After the evaluation job is finished, we can download the evaluation results from the S3 bucket. Lighteval will save the results and generations in the output_dir. The results are savedas json and include detailed information about each task and the model's performance. The results are available in the results key."
      ],
      "metadata": {
        "id": "COjNJ7XbNChj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import json\n",
        "import io\n",
        "import os\n",
        "from sagemaker.s3 import S3Downloader\n",
        "\n",
        "\n",
        "# download results from s3\n",
        "results_tar = S3Downloader.read_bytes(huggingface_estimator.model_data)\n",
        "model_id = hyperparameters[\"model_args\"].split(\"=\")[1]\n",
        "result={}\n",
        "\n",
        "# Use tarfile to open the tar content directly from bytes\n",
        "with tarfile.open(fileobj=io.BytesIO(results_tar), mode=\"r:gz\") as tar:\n",
        "    # Iterate over items in tar archive to find your json file by its path\n",
        "    for member in tar.getmembers():\n",
        "        # get path of results based on model id used to evaluate\n",
        "        if os.path.join(\"details\", model_id) in member.name and member.name.endswith('.json'):\n",
        "            # Extract the file content\n",
        "            f = tar.extractfile(member)\n",
        "            if f is not None:\n",
        "                content = f.read()\n",
        "                result = json.loads(content)\n",
        "                break\n",
        "\n",
        "# print results\n",
        "print(result[\"results\"])"
      ],
      "metadata": {
        "id": "e3Qst5oOL6BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "\n",
        "`{'lighteval|truthfulqa:mc|0': {'truthfulqa_mc1': 0.40636474908200737,\n",
        "'truthfulqa_mc1_stderr': 0.017193835812093897,\n",
        "'truthfulqa_mc2': 0.5747003398184238,\n",
        "'truthfulqa_mc2_stderr': 0.015742356478301463}}`\n",
        "\n",
        "In our test we achieved a mc1 score of `40.6%` and an mc2 score of `57.47%`. The mc2 is the score used in the Open LLM Leaderboard. Zephyr 7B achieved a mc2 score of `57.47%` on the TruthfulQA benchmark, which is identical to the score on the Open LLM Leaderboard. The evaluation on Truthfulqa took 999 seconds.\n",
        "The `ml.g5.4xlarge` instance we used costs `$2.03` per hour for on-demand usage. As a result, the total cost for evaluating Zephyr 7B on Truthfulqa was `$0.56`."
      ],
      "metadata": {
        "id": "ykDbIwVHNRDS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ao-3fBhkL6E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WR8uw6Z_PkQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Zc0UvtDPkVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install"
      ],
      "metadata": {
        "id": "tSjRiO85N83r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/lighteval.git\n",
        "%cd lighteval"
      ],
      "metadata": {
        "id": "7dpf4MX9N7v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conda create -n lighteval python=3.10 && conda activate lighteval\n",
        "# pip install .\n",
        "# pip install '.[accelerate,quantization,adapters]'\n",
        "# huggingface-cli login"
      ],
      "metadata": {
        "id": "eL3O1VhAN70K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate a model on one or more GPUs (recommended)\n",
        "\n",
        "To evaluate a model on one or more GPUs, first create a multi-gpu config by running:"
      ],
      "metadata": {
        "id": "0tjefoVROidd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config"
      ],
      "metadata": {
        "id": "eM74INvmN73i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --multi_gpu --num_processes=8 run_evals_accelerate.py \\\n",
        "    --model_args \"pretrained=gpt2\" \\\n",
        "    --tasks \"lighteval|truthfulqa:mc|0|0\" \\\n",
        "    --override_batch_size 1 \\\n",
        "    --output_dir=\"./evals/\""
      ],
      "metadata": {
        "id": "n_IAEZIQN76q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --multi_gpu --num_processes=8 run_evals_accelerate.py \\\n",
        "    --model_args \"pretrained=gpt2\" \\\n",
        "    --tasks \"lighteval|truthfulqa:mc|0|0,lighteval|gsm8k|0|0\" \\\n",
        "    --override_batch_size 1 \\\n",
        "    --output_dir=\"./evals/\""
      ],
      "metadata": {
        "id": "nVgYUL8pOvnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_evals_accelerate.py \\\n",
        "    --model_args \"pretrained=HuggingFaceH4/zephyr-7b-beta\" \\\n",
        "    --use_chat_template \\\n",
        "    --tasks \"custom|ifeval|0|0\" \\\n",
        "    --custom_tasks \"tasks_examples/custom_tasks_with_custom_metrics/ifeval/ifeval.py\" \\\n",
        "    --output_dir output_dir"
      ],
      "metadata": {
        "id": "vMe_bga_PXP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate a model on the Open LLM Leaderboard benchmarks\n",
        "\n",
        "To evaluate a model on all the benchmarks of the Open LLM Leaderboard using a single node of 8 GPUs, run:\n",
        "\n",
        "# PP=2, DP=4 - good for models < 70B params\n",
        "accelerate launch --multi_gpu --num_processes=4 run_evals_accelerate.py \\\n",
        "    --model_args=\"pretrained=<path to model on the hub>\" \\\n",
        "    --model_parallel \\\n",
        "    --tasks <task parameters> \\\n",
        "    --output_dir output_dir\n",
        "\n",
        "# PP=4, DP=2 - good for huge models >= 70B params\n",
        "accelerate launch --multi_gpu --num_processes=2 run_evals_accelerate.py \\\n",
        "    --model_args=\"pretrained=<path to model on the hub>\" \\\n",
        "    --model_parallel \\\n",
        "    --tasks <task parameters> \\\n",
        "    --output_dir output_dir"
      ],
      "metadata": {
        "id": "2HLqaTOZO8X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch \\\n",
        "--multi_gpu --num_processes=8 run_evals_accelerate.py \\\n",
        "--model_args \"pretrained=<model name>\" \\\n",
        "--tasks tasks_examples/open_llm_leaderboard_tasks.txt \\\n",
        "--override_batch_size 1 \\\n",
        "--output_dir=\"./evals/\""
      ],
      "metadata": {
        "id": "D4_P1B_nOvqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6K03PHWEOvt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZLUIgBxpOvxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3dBM5STOv0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}