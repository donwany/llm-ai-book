{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-_k4j9wm5GD"
      },
      "source": [
        "## AQLM transformers integration example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6egoxPVyckBF"
      },
      "source": [
        "**Install the `aqlm` library**\n",
        "- The only extra dependency to run AQLM models.\n",
        "- Add `[gpu]` to install the required CUDA specific dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A584OAwRWGks"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install aqlm[gpu]>=1.0.1\n",
        "!pip install accelerate>=0.27.0\n",
        "!pip install transformers>=4.38.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTfcs4lrc1x4"
      },
      "source": [
        "**Load the model as usual**\n",
        "\n",
        "The tokenizer is just a normal `Mixtral` tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lecaItWkVpIC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf\",\n",
        "    torch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39QpRiPbcBYa"
      },
      "source": [
        "Do a few forward passes to load CUDA and automatically compile the kernels. It's done separately here for it not to affect the generation speed benchmark below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii-mWRdQZCOF",
        "outputId": "98c14710-7772-43a4-fa83-53c92000f1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "output = quantized_model.generate(tokenizer(\"\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOQfeb_ScIyb"
      },
      "source": [
        "**Measure generation speed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyl4uCxTdmKi",
        "outputId": "22be738e-8ec7-4f29-8f15-515d9e67bf87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.5 s, sys: 0 ns, total: 21.5 s\n",
            "Wall time: 21.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "output = quantized_model.generate(tokenizer(\"I'm AQLM, \", return_tensors=\"pt\")[\"input_ids\"].cuda(), min_new_tokens=128, max_new_tokens=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G5E4tmVdpLF"
      },
      "source": [
        "Note that `transformers` generation is not the fastest implementation and it's heavily influenced by CPU capabilities of _Google Colab_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvShqlguccep"
      },
      "source": [
        "**Check that the output is what one would expect from Mixtral**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsOmDVBvXobJ",
        "outputId": "27ec529f-9d2d-4385-dca0-95e90cf81b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> I'm AQLM, 20 years old, and I'm a student at the University of California, Berkeley. I'm a member of the Berkeley Student Union, and I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student Union. I'm a member of the Berkeley Student\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6SuiDKYmpCq"
      },
      "source": [
        "**Check peak memory usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDOtpnJGizsx",
        "outputId": "03029b5a-cdb6-43bd-def8-9e7aa5f94bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peak memory usage: 13.68 Gb\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"Peak memory usage: {torch.cuda.max_memory_allocated()*1e-9:.2f} Gb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNAxWzMWmr3T"
      },
      "source": [
        "Indeed, it's ~2 bits per model weight."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}