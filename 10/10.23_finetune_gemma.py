# Create a new Conda environment named 'gemma' with Python 3.11
# conda create -n gemma python=3.11

# Activate the 'gemma' Conda environment
# conda activate gemma

# Install the necessary Python packages and dependencies
# pip install -U bitsandbytes==0.42.0 peft==0.8.2 trl==0.7.10 accelerate==0.27.1 datasets==2.17.0 transformers==4.38.0

# Export your Hugging Face token for authentication
# export HF_TOKEN=xxxxxxxxxxxx

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig
from datasets import load_dataset
from trl import SFTTrainer


def main():
    # Configuration
    model_id = "google/gemma-7b"  # Model identifier from Hugging Face Hub

    # Configure Bits and Bytes for 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # Enable 4-bit quantization
        bnb_4bit_quant_type="nf4",  # Quantization type
        bnb_4bit_compute_dtype=torch.bfloat16  # Compute data type
    )

    # Load the tokenizer for the model
    tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])

    # Configure LoRA (Low-Rank Adaptation) for the model
    lora_config = LoraConfig(
        r=8,  # Rank of the LoRA components
        target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"],
        # Targeted modules for LoRA
        task_type="CAUSAL_LM"  # Task type for the model
    )

    # Load the pre-trained model with quantization configuration and device mapping
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,  # Apply Bits and Bytes configuration
        device_map={"": 0},  # Map model to GPU device 0
        token=os.environ['HF_TOKEN']  # Authentication token
    )

    # Load and preprocess the dataset
    data = load_dataset("Abirate/english_quotes")  # Load dataset from Hugging Face Hub
    data = data.map(lambda samples: tokenizer(samples["quote"]), batched=True)  # Tokenize the quotes

    # Print a sample text generated by the model before training
    print("Before training\n")
    generate_text(model, tokenizer, "Quote: Imagination is more")

    # Configure and initialize the trainer for fine-tuning
    trainer = SFTTrainer(
        model=model,
        train_dataset=data["train"],  # Training dataset
        max_seq_length=1024,  # Maximum sequence length for training
        args=TrainingArguments(
            per_device_train_batch_size=1,  # Batch size per device
            gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps
            warmup_steps=2,  # Number of warmup steps
            max_steps=10,  # Total number of training steps
            learning_rate=2e-4,  # Learning rate
            fp16=True,  # Enable mixed precision training
            logging_steps=1,  # Log every step
            output_dir="outputs",  # Directory to save model outputs
            optim="paged_adamw_8bit"  # Optimizer with 8-bit precision
        ),
        peft_config=lora_config,
        formatting_func=lambda example: [f"Quote: {example['quote'][0]}\nAuthor: {example['author'][0]}"]
        # Formatting function for the dataset
    )

    # Train the model
    trainer.train()

    # Print a sample text generated by the model after training
    print("\n ######## \nAfter training\n")
    generate_text(model, tokenizer, "Quote: Imagination is")

    # Save the trained model to the output directory
    model.save_pretrained("outputs")


def generate_text(model, tokenizer, text):
    """
    Generate text using the model and tokenizer.
    Args:
        model: The model used for text generation.
        tokenizer: The tokenizer used to process the input text.
        text: The input text prompt for generation.
    Returns:
        None
    """
    inputs = tokenizer(text, return_tensors="pt").to("cuda:0")  # Tokenize and move to GPU
    outputs = model.generate(**inputs, max_new_tokens=20)  # Generate new text
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))  # Decode and print the generated text


if __name__ == "__main__":
    main()
